{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification,ViTConfig\n",
    "from dataclasses import dataclass\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\") #finetuned on imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf_1=model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.41.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf=ViTConfig()\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ViT_Config():\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    embed_dim=768\n",
    "    ff_dim=768*4 \n",
    "    num_heads=12\n",
    "    layers=12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "for k,v in model_hf.items():\n",
    "    x=x+1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ModuleDict.__init__() got an unexpected keyword argument 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 158\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    157\u001b[0m conf \u001b[38;5;241m=\u001b[39m ViT_Config()\n\u001b[0;32m--> 158\u001b[0m vit \u001b[38;5;241m=\u001b[39m \u001b[43mViT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/vit-base-patch16-224\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# model = ViT(conf)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[140], line 131\u001b[0m, in \u001b[0;36mViT.from_pretrained\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m    125\u001b[0m patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m    126\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mdict\u001b[39m(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,  ff_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m,  num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m),\n\u001b[1;32m    128\u001b[0m }[model_type]\n\u001b[0;32m--> 131\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m sd \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sd) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(sd_hf), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmismatch state dict, maybe you forgot to consider something\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[140], line 89\u001b[0m, in \u001b[0;36mViT.__init__\u001b[0;34m(self, image_size, patch_size, embed_dim, ff_dim, num_heads, layers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels, \n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \n\u001b[1;32m     80\u001b[0m     kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size, \n\u001b[1;32m     81\u001b[0m     stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleDict(\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m---> 89\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModuleDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModuleList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: ModuleDict.__init__() got an unexpected keyword argument 'layer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, max_length, embed_dim, ff_dim, num_heads, dropout=0.1):\n",
    "\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisble by num_heads\"\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dp = dropout\n",
    "\n",
    "        #derv:\n",
    "        self.head_size = self.embed_dim // self.num_heads\n",
    "\n",
    "        #attention blocks\n",
    "        self.attention=nn.ModuleDict(dict(\n",
    "            attention=nn.ModuleDict(dict(\n",
    "                                    query = nn.Linear(self.embed_dim, self.embed_dim),\n",
    "                                    key = nn.Linear(self.embed_dim, self.embed_dim),\n",
    "                                    value = nn.Linear(self.embed_dim, self.embed_dim),\n",
    "                                    )),\n",
    "            \n",
    "            output=nn.ModuleDict(dict(\n",
    "                                dense=nn.Linear(self.embed_dim, self.ff_dim)\n",
    "                                    ))\n",
    "        ))\n",
    "\n",
    "        self.intermediate=nn.ModuleDict(dict(\n",
    "                        dense=nn.Linear(self.embed_dim, self.ff_dim)\n",
    "        ))\n",
    "        self.output=nn.ModuleDict(dict(\n",
    "                        dense=nn.Linear(self.embed_dim, self.ff_dim),\n",
    "        ))\n",
    "        \n",
    "        \n",
    "        #after attn and ff blocks\n",
    "        # self.dropout = nn.Dropout(self.dp, inplace=True)\n",
    "\n",
    "        #depends\n",
    "        self.layernorm_before = nn.LayerNorm(self.embed_dim)\n",
    "        self.layernorm_after = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "\n",
    "class ViT_Config:\n",
    "    def __init__(self):\n",
    "        self.image_size = 224\n",
    "        self.patch_size = 16\n",
    "        self.embed_dim = 768\n",
    "        self.ff_dim = 3072\n",
    "        self.num_heads = 12\n",
    "        self.layers = 12\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, ff_dim, num_heads, layers):\n",
    "        super(ViT, self).__init__()\n",
    "        assert image_size % patch_size == 0\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.layers = layers\n",
    "    \n",
    "        self.num_channels = 3\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        self.register_buffer(\"cls_token\", torch.ones(1, 1, self.embed_dim))\n",
    "\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, self.num_patches+1, self.embed_dim))\n",
    "\n",
    "        #For patch embedding fn:\n",
    "        self.projection = nn.Conv2d(\n",
    "            self.num_channels, \n",
    "            self.embed_dim, \n",
    "            kernel_size=self.patch_size, \n",
    "            stride=self.patch_size\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(0.1, inplace=True)\n",
    "        self.vit=nn.ModuleDict(dict(\n",
    "            encoder = nn.ModuleDict(layer=nn.ModuleList([\n",
    "                                        TransformerBlock(\n",
    "                                            self.num_patches, \n",
    "                                            self.embed_dim, \n",
    "                                            self.ff_dim, \n",
    "                                            self.num_heads\n",
    "                                        ) for _ in range(self.layers)\n",
    "                                            ])\n",
    "        )))\n",
    "        \n",
    "\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        self.head = nn.Linear(self.embed_dim, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.projection(x).flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.ln_f(x[:, 0])\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        assert model_type in ('google/vit-base-patch16-224')\n",
    "\n",
    "        model_hf = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\") #finetuned on imagenet\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        image_size = 224\n",
    "        patch_size = 16\n",
    "        config = {\n",
    "            'google/vit-base-patch16-224' : dict(embed_dim=768,  ff_dim=768*4,  num_heads=12, layers=12),\n",
    "        }[model_type]\n",
    "\n",
    "\n",
    "        model = cls(image_size, patch_size, **config)\n",
    "        sd = model.state_dict()\n",
    "\n",
    "        assert len(sd) == len(sd_hf), \"mismatch state dict, maybe you forgot to consider something\"\n",
    "\n",
    "        mapping = {\n",
    "            'vit.embeddings.cls_token': 'cls_token',\n",
    "            'vit.embeddings.position_embeddings': 'position_embeddings',\n",
    "            'vit.embeddings.patch_embeddings.projection.weight': 'projection.weight',\n",
    "            'vit.embeddings.patch_embeddings.projection.bias': 'projection.bias',\n",
    "            'vit.layernorm.weight': 'ln_f.weight',\n",
    "            'vit.layernorm.bias': 'ln_f.bias',\n",
    "            'classifier.weight': 'head.weight',\n",
    "            'classifier.bias': 'head.bias'\n",
    "        }\n",
    "        # assert len(mapping.keys()) == len(sd_hf.keys()), \"mismatch mapping between the models\"\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        print(\"Importing ViT\")\n",
    "        for k in tqdm(sd_hf):\n",
    "            kn = mapping[k]\n",
    "            assert sd_hf[k].shape == sd[kn].shape;\n",
    "            with torch.no_grad():\n",
    "                sd[kn].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "conf = ViT_Config()\n",
    "vit = ViT.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# model = ViT(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_embeddings\n",
      "cls_token\n",
      "projection.weight\n",
      "projection.bias\n",
      "vit.encoder_layers.0.attention.attention.query.weight\n",
      "vit.encoder_layers.0.attention.attention.query.bias\n",
      "vit.encoder_layers.0.attention.attention.key.weight\n",
      "vit.encoder_layers.0.attention.attention.key.bias\n",
      "vit.encoder_layers.0.attention.attention.value.weight\n",
      "vit.encoder_layers.0.attention.attention.value.bias\n",
      "vit.encoder_layers.0.attention.output.dense.weight\n",
      "vit.encoder_layers.0.attention.output.dense.bias\n",
      "vit.encoder_layers.0.intermediate.dense.weight\n",
      "vit.encoder_layers.0.intermediate.dense.bias\n",
      "vit.encoder_layers.0.output.dense.weight\n",
      "vit.encoder_layers.0.output.dense.bias\n",
      "vit.encoder_layers.0.layernorm_before.weight\n",
      "vit.encoder_layers.0.layernorm_before.bias\n",
      "vit.encoder_layers.0.layernorm_after.weight\n",
      "vit.encoder_layers.0.layernorm_after.bias\n",
      "vit.encoder_layers.1.attention.attention.query.weight\n",
      "vit.encoder_layers.1.attention.attention.query.bias\n",
      "vit.encoder_layers.1.attention.attention.key.weight\n",
      "vit.encoder_layers.1.attention.attention.key.bias\n",
      "vit.encoder_layers.1.attention.attention.value.weight\n",
      "vit.encoder_layers.1.attention.attention.value.bias\n",
      "vit.encoder_layers.1.attention.output.dense.weight\n",
      "vit.encoder_layers.1.attention.output.dense.bias\n",
      "vit.encoder_layers.1.intermediate.dense.weight\n",
      "vit.encoder_layers.1.intermediate.dense.bias\n",
      "vit.encoder_layers.1.output.dense.weight\n",
      "vit.encoder_layers.1.output.dense.bias\n",
      "vit.encoder_layers.1.layernorm_before.weight\n",
      "vit.encoder_layers.1.layernorm_before.bias\n",
      "vit.encoder_layers.1.layernorm_after.weight\n",
      "vit.encoder_layers.1.layernorm_after.bias\n",
      "vit.encoder_layers.2.attention.attention.query.weight\n",
      "vit.encoder_layers.2.attention.attention.query.bias\n",
      "vit.encoder_layers.2.attention.attention.key.weight\n",
      "vit.encoder_layers.2.attention.attention.key.bias\n",
      "vit.encoder_layers.2.attention.attention.value.weight\n",
      "vit.encoder_layers.2.attention.attention.value.bias\n",
      "vit.encoder_layers.2.attention.output.dense.weight\n",
      "vit.encoder_layers.2.attention.output.dense.bias\n",
      "vit.encoder_layers.2.intermediate.dense.weight\n",
      "vit.encoder_layers.2.intermediate.dense.bias\n",
      "vit.encoder_layers.2.output.dense.weight\n",
      "vit.encoder_layers.2.output.dense.bias\n",
      "vit.encoder_layers.2.layernorm_before.weight\n",
      "vit.encoder_layers.2.layernorm_before.bias\n",
      "vit.encoder_layers.2.layernorm_after.weight\n",
      "vit.encoder_layers.2.layernorm_after.bias\n",
      "vit.encoder_layers.3.attention.attention.query.weight\n",
      "vit.encoder_layers.3.attention.attention.query.bias\n",
      "vit.encoder_layers.3.attention.attention.key.weight\n",
      "vit.encoder_layers.3.attention.attention.key.bias\n",
      "vit.encoder_layers.3.attention.attention.value.weight\n",
      "vit.encoder_layers.3.attention.attention.value.bias\n",
      "vit.encoder_layers.3.attention.output.dense.weight\n",
      "vit.encoder_layers.3.attention.output.dense.bias\n",
      "vit.encoder_layers.3.intermediate.dense.weight\n",
      "vit.encoder_layers.3.intermediate.dense.bias\n",
      "vit.encoder_layers.3.output.dense.weight\n",
      "vit.encoder_layers.3.output.dense.bias\n",
      "vit.encoder_layers.3.layernorm_before.weight\n",
      "vit.encoder_layers.3.layernorm_before.bias\n",
      "vit.encoder_layers.3.layernorm_after.weight\n",
      "vit.encoder_layers.3.layernorm_after.bias\n",
      "vit.encoder_layers.4.attention.attention.query.weight\n",
      "vit.encoder_layers.4.attention.attention.query.bias\n",
      "vit.encoder_layers.4.attention.attention.key.weight\n",
      "vit.encoder_layers.4.attention.attention.key.bias\n",
      "vit.encoder_layers.4.attention.attention.value.weight\n",
      "vit.encoder_layers.4.attention.attention.value.bias\n",
      "vit.encoder_layers.4.attention.output.dense.weight\n",
      "vit.encoder_layers.4.attention.output.dense.bias\n",
      "vit.encoder_layers.4.intermediate.dense.weight\n",
      "vit.encoder_layers.4.intermediate.dense.bias\n",
      "vit.encoder_layers.4.output.dense.weight\n",
      "vit.encoder_layers.4.output.dense.bias\n",
      "vit.encoder_layers.4.layernorm_before.weight\n",
      "vit.encoder_layers.4.layernorm_before.bias\n",
      "vit.encoder_layers.4.layernorm_after.weight\n",
      "vit.encoder_layers.4.layernorm_after.bias\n",
      "vit.encoder_layers.5.attention.attention.query.weight\n",
      "vit.encoder_layers.5.attention.attention.query.bias\n",
      "vit.encoder_layers.5.attention.attention.key.weight\n",
      "vit.encoder_layers.5.attention.attention.key.bias\n",
      "vit.encoder_layers.5.attention.attention.value.weight\n",
      "vit.encoder_layers.5.attention.attention.value.bias\n",
      "vit.encoder_layers.5.attention.output.dense.weight\n",
      "vit.encoder_layers.5.attention.output.dense.bias\n",
      "vit.encoder_layers.5.intermediate.dense.weight\n",
      "vit.encoder_layers.5.intermediate.dense.bias\n",
      "vit.encoder_layers.5.output.dense.weight\n",
      "vit.encoder_layers.5.output.dense.bias\n",
      "vit.encoder_layers.5.layernorm_before.weight\n",
      "vit.encoder_layers.5.layernorm_before.bias\n",
      "vit.encoder_layers.5.layernorm_after.weight\n",
      "vit.encoder_layers.5.layernorm_after.bias\n",
      "vit.encoder_layers.6.attention.attention.query.weight\n",
      "vit.encoder_layers.6.attention.attention.query.bias\n",
      "vit.encoder_layers.6.attention.attention.key.weight\n",
      "vit.encoder_layers.6.attention.attention.key.bias\n",
      "vit.encoder_layers.6.attention.attention.value.weight\n",
      "vit.encoder_layers.6.attention.attention.value.bias\n",
      "vit.encoder_layers.6.attention.output.dense.weight\n",
      "vit.encoder_layers.6.attention.output.dense.bias\n",
      "vit.encoder_layers.6.intermediate.dense.weight\n",
      "vit.encoder_layers.6.intermediate.dense.bias\n",
      "vit.encoder_layers.6.output.dense.weight\n",
      "vit.encoder_layers.6.output.dense.bias\n",
      "vit.encoder_layers.6.layernorm_before.weight\n",
      "vit.encoder_layers.6.layernorm_before.bias\n",
      "vit.encoder_layers.6.layernorm_after.weight\n",
      "vit.encoder_layers.6.layernorm_after.bias\n",
      "vit.encoder_layers.7.attention.attention.query.weight\n",
      "vit.encoder_layers.7.attention.attention.query.bias\n",
      "vit.encoder_layers.7.attention.attention.key.weight\n",
      "vit.encoder_layers.7.attention.attention.key.bias\n",
      "vit.encoder_layers.7.attention.attention.value.weight\n",
      "vit.encoder_layers.7.attention.attention.value.bias\n",
      "vit.encoder_layers.7.attention.output.dense.weight\n",
      "vit.encoder_layers.7.attention.output.dense.bias\n",
      "vit.encoder_layers.7.intermediate.dense.weight\n",
      "vit.encoder_layers.7.intermediate.dense.bias\n",
      "vit.encoder_layers.7.output.dense.weight\n",
      "vit.encoder_layers.7.output.dense.bias\n",
      "vit.encoder_layers.7.layernorm_before.weight\n",
      "vit.encoder_layers.7.layernorm_before.bias\n",
      "vit.encoder_layers.7.layernorm_after.weight\n",
      "vit.encoder_layers.7.layernorm_after.bias\n",
      "vit.encoder_layers.8.attention.attention.query.weight\n",
      "vit.encoder_layers.8.attention.attention.query.bias\n",
      "vit.encoder_layers.8.attention.attention.key.weight\n",
      "vit.encoder_layers.8.attention.attention.key.bias\n",
      "vit.encoder_layers.8.attention.attention.value.weight\n",
      "vit.encoder_layers.8.attention.attention.value.bias\n",
      "vit.encoder_layers.8.attention.output.dense.weight\n",
      "vit.encoder_layers.8.attention.output.dense.bias\n",
      "vit.encoder_layers.8.intermediate.dense.weight\n",
      "vit.encoder_layers.8.intermediate.dense.bias\n",
      "vit.encoder_layers.8.output.dense.weight\n",
      "vit.encoder_layers.8.output.dense.bias\n",
      "vit.encoder_layers.8.layernorm_before.weight\n",
      "vit.encoder_layers.8.layernorm_before.bias\n",
      "vit.encoder_layers.8.layernorm_after.weight\n",
      "vit.encoder_layers.8.layernorm_after.bias\n",
      "vit.encoder_layers.9.attention.attention.query.weight\n",
      "vit.encoder_layers.9.attention.attention.query.bias\n",
      "vit.encoder_layers.9.attention.attention.key.weight\n",
      "vit.encoder_layers.9.attention.attention.key.bias\n",
      "vit.encoder_layers.9.attention.attention.value.weight\n",
      "vit.encoder_layers.9.attention.attention.value.bias\n",
      "vit.encoder_layers.9.attention.output.dense.weight\n",
      "vit.encoder_layers.9.attention.output.dense.bias\n",
      "vit.encoder_layers.9.intermediate.dense.weight\n",
      "vit.encoder_layers.9.intermediate.dense.bias\n",
      "vit.encoder_layers.9.output.dense.weight\n",
      "vit.encoder_layers.9.output.dense.bias\n",
      "vit.encoder_layers.9.layernorm_before.weight\n",
      "vit.encoder_layers.9.layernorm_before.bias\n",
      "vit.encoder_layers.9.layernorm_after.weight\n",
      "vit.encoder_layers.9.layernorm_after.bias\n",
      "vit.encoder_layers.10.attention.attention.query.weight\n",
      "vit.encoder_layers.10.attention.attention.query.bias\n",
      "vit.encoder_layers.10.attention.attention.key.weight\n",
      "vit.encoder_layers.10.attention.attention.key.bias\n",
      "vit.encoder_layers.10.attention.attention.value.weight\n",
      "vit.encoder_layers.10.attention.attention.value.bias\n",
      "vit.encoder_layers.10.attention.output.dense.weight\n",
      "vit.encoder_layers.10.attention.output.dense.bias\n",
      "vit.encoder_layers.10.intermediate.dense.weight\n",
      "vit.encoder_layers.10.intermediate.dense.bias\n",
      "vit.encoder_layers.10.output.dense.weight\n",
      "vit.encoder_layers.10.output.dense.bias\n",
      "vit.encoder_layers.10.layernorm_before.weight\n",
      "vit.encoder_layers.10.layernorm_before.bias\n",
      "vit.encoder_layers.10.layernorm_after.weight\n",
      "vit.encoder_layers.10.layernorm_after.bias\n",
      "vit.encoder_layers.11.attention.attention.query.weight\n",
      "vit.encoder_layers.11.attention.attention.query.bias\n",
      "vit.encoder_layers.11.attention.attention.key.weight\n",
      "vit.encoder_layers.11.attention.attention.key.bias\n",
      "vit.encoder_layers.11.attention.attention.value.weight\n",
      "vit.encoder_layers.11.attention.attention.value.bias\n",
      "vit.encoder_layers.11.attention.output.dense.weight\n",
      "vit.encoder_layers.11.attention.output.dense.bias\n",
      "vit.encoder_layers.11.intermediate.dense.weight\n",
      "vit.encoder_layers.11.intermediate.dense.bias\n",
      "vit.encoder_layers.11.output.dense.weight\n",
      "vit.encoder_layers.11.output.dense.bias\n",
      "vit.encoder_layers.11.layernorm_before.weight\n",
      "vit.encoder_layers.11.layernorm_before.bias\n",
      "vit.encoder_layers.11.layernorm_after.weight\n",
      "vit.encoder_layers.11.layernorm_after.bias\n",
      "ln_f.weight\n",
      "ln_f.bias\n",
      "head.weight\n",
      "head.bias\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for k in model.state_dict().keys():\n",
    "    print(k)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.embeddings.cls_token\n",
      "vit.embeddings.position_embeddings\n",
      "vit.embeddings.patch_embeddings.projection.weight\n",
      "vit.embeddings.patch_embeddings.projection.bias\n",
      "vit.encoder.layer.0.attention.attention.query.weight\n",
      "vit.encoder.layer.0.attention.attention.query.bias\n",
      "vit.encoder.layer.0.attention.attention.key.weight\n",
      "vit.encoder.layer.0.attention.attention.key.bias\n",
      "vit.encoder.layer.0.attention.attention.value.weight\n",
      "vit.encoder.layer.0.attention.attention.value.bias\n",
      "vit.encoder.layer.0.attention.output.dense.weight\n",
      "vit.encoder.layer.0.attention.output.dense.bias\n",
      "vit.encoder.layer.0.intermediate.dense.weight\n",
      "vit.encoder.layer.0.intermediate.dense.bias\n",
      "vit.encoder.layer.0.output.dense.weight\n",
      "vit.encoder.layer.0.output.dense.bias\n",
      "vit.encoder.layer.0.layernorm_before.weight\n",
      "vit.encoder.layer.0.layernorm_before.bias\n",
      "vit.encoder.layer.0.layernorm_after.weight\n",
      "vit.encoder.layer.0.layernorm_after.bias\n",
      "vit.encoder.layer.1.attention.attention.query.weight\n",
      "vit.encoder.layer.1.attention.attention.query.bias\n",
      "vit.encoder.layer.1.attention.attention.key.weight\n",
      "vit.encoder.layer.1.attention.attention.key.bias\n",
      "vit.encoder.layer.1.attention.attention.value.weight\n",
      "vit.encoder.layer.1.attention.attention.value.bias\n",
      "vit.encoder.layer.1.attention.output.dense.weight\n",
      "vit.encoder.layer.1.attention.output.dense.bias\n",
      "vit.encoder.layer.1.intermediate.dense.weight\n",
      "vit.encoder.layer.1.intermediate.dense.bias\n",
      "vit.encoder.layer.1.output.dense.weight\n",
      "vit.encoder.layer.1.output.dense.bias\n",
      "vit.encoder.layer.1.layernorm_before.weight\n",
      "vit.encoder.layer.1.layernorm_before.bias\n",
      "vit.encoder.layer.1.layernorm_after.weight\n",
      "vit.encoder.layer.1.layernorm_after.bias\n",
      "vit.encoder.layer.2.attention.attention.query.weight\n",
      "vit.encoder.layer.2.attention.attention.query.bias\n",
      "vit.encoder.layer.2.attention.attention.key.weight\n",
      "vit.encoder.layer.2.attention.attention.key.bias\n",
      "vit.encoder.layer.2.attention.attention.value.weight\n",
      "vit.encoder.layer.2.attention.attention.value.bias\n",
      "vit.encoder.layer.2.attention.output.dense.weight\n",
      "vit.encoder.layer.2.attention.output.dense.bias\n",
      "vit.encoder.layer.2.intermediate.dense.weight\n",
      "vit.encoder.layer.2.intermediate.dense.bias\n",
      "vit.encoder.layer.2.output.dense.weight\n",
      "vit.encoder.layer.2.output.dense.bias\n",
      "vit.encoder.layer.2.layernorm_before.weight\n",
      "vit.encoder.layer.2.layernorm_before.bias\n",
      "vit.encoder.layer.2.layernorm_after.weight\n",
      "vit.encoder.layer.2.layernorm_after.bias\n",
      "vit.encoder.layer.3.attention.attention.query.weight\n",
      "vit.encoder.layer.3.attention.attention.query.bias\n",
      "vit.encoder.layer.3.attention.attention.key.weight\n",
      "vit.encoder.layer.3.attention.attention.key.bias\n",
      "vit.encoder.layer.3.attention.attention.value.weight\n",
      "vit.encoder.layer.3.attention.attention.value.bias\n",
      "vit.encoder.layer.3.attention.output.dense.weight\n",
      "vit.encoder.layer.3.attention.output.dense.bias\n",
      "vit.encoder.layer.3.intermediate.dense.weight\n",
      "vit.encoder.layer.3.intermediate.dense.bias\n",
      "vit.encoder.layer.3.output.dense.weight\n",
      "vit.encoder.layer.3.output.dense.bias\n",
      "vit.encoder.layer.3.layernorm_before.weight\n",
      "vit.encoder.layer.3.layernorm_before.bias\n",
      "vit.encoder.layer.3.layernorm_after.weight\n",
      "vit.encoder.layer.3.layernorm_after.bias\n",
      "vit.encoder.layer.4.attention.attention.query.weight\n",
      "vit.encoder.layer.4.attention.attention.query.bias\n",
      "vit.encoder.layer.4.attention.attention.key.weight\n",
      "vit.encoder.layer.4.attention.attention.key.bias\n",
      "vit.encoder.layer.4.attention.attention.value.weight\n",
      "vit.encoder.layer.4.attention.attention.value.bias\n",
      "vit.encoder.layer.4.attention.output.dense.weight\n",
      "vit.encoder.layer.4.attention.output.dense.bias\n",
      "vit.encoder.layer.4.intermediate.dense.weight\n",
      "vit.encoder.layer.4.intermediate.dense.bias\n",
      "vit.encoder.layer.4.output.dense.weight\n",
      "vit.encoder.layer.4.output.dense.bias\n",
      "vit.encoder.layer.4.layernorm_before.weight\n",
      "vit.encoder.layer.4.layernorm_before.bias\n",
      "vit.encoder.layer.4.layernorm_after.weight\n",
      "vit.encoder.layer.4.layernorm_after.bias\n",
      "vit.encoder.layer.5.attention.attention.query.weight\n",
      "vit.encoder.layer.5.attention.attention.query.bias\n",
      "vit.encoder.layer.5.attention.attention.key.weight\n",
      "vit.encoder.layer.5.attention.attention.key.bias\n",
      "vit.encoder.layer.5.attention.attention.value.weight\n",
      "vit.encoder.layer.5.attention.attention.value.bias\n",
      "vit.encoder.layer.5.attention.output.dense.weight\n",
      "vit.encoder.layer.5.attention.output.dense.bias\n",
      "vit.encoder.layer.5.intermediate.dense.weight\n",
      "vit.encoder.layer.5.intermediate.dense.bias\n",
      "vit.encoder.layer.5.output.dense.weight\n",
      "vit.encoder.layer.5.output.dense.bias\n",
      "vit.encoder.layer.5.layernorm_before.weight\n",
      "vit.encoder.layer.5.layernorm_before.bias\n",
      "vit.encoder.layer.5.layernorm_after.weight\n",
      "vit.encoder.layer.5.layernorm_after.bias\n",
      "vit.encoder.layer.6.attention.attention.query.weight\n",
      "vit.encoder.layer.6.attention.attention.query.bias\n",
      "vit.encoder.layer.6.attention.attention.key.weight\n",
      "vit.encoder.layer.6.attention.attention.key.bias\n",
      "vit.encoder.layer.6.attention.attention.value.weight\n",
      "vit.encoder.layer.6.attention.attention.value.bias\n",
      "vit.encoder.layer.6.attention.output.dense.weight\n",
      "vit.encoder.layer.6.attention.output.dense.bias\n",
      "vit.encoder.layer.6.intermediate.dense.weight\n",
      "vit.encoder.layer.6.intermediate.dense.bias\n",
      "vit.encoder.layer.6.output.dense.weight\n",
      "vit.encoder.layer.6.output.dense.bias\n",
      "vit.encoder.layer.6.layernorm_before.weight\n",
      "vit.encoder.layer.6.layernorm_before.bias\n",
      "vit.encoder.layer.6.layernorm_after.weight\n",
      "vit.encoder.layer.6.layernorm_after.bias\n",
      "vit.encoder.layer.7.attention.attention.query.weight\n",
      "vit.encoder.layer.7.attention.attention.query.bias\n",
      "vit.encoder.layer.7.attention.attention.key.weight\n",
      "vit.encoder.layer.7.attention.attention.key.bias\n",
      "vit.encoder.layer.7.attention.attention.value.weight\n",
      "vit.encoder.layer.7.attention.attention.value.bias\n",
      "vit.encoder.layer.7.attention.output.dense.weight\n",
      "vit.encoder.layer.7.attention.output.dense.bias\n",
      "vit.encoder.layer.7.intermediate.dense.weight\n",
      "vit.encoder.layer.7.intermediate.dense.bias\n",
      "vit.encoder.layer.7.output.dense.weight\n",
      "vit.encoder.layer.7.output.dense.bias\n",
      "vit.encoder.layer.7.layernorm_before.weight\n",
      "vit.encoder.layer.7.layernorm_before.bias\n",
      "vit.encoder.layer.7.layernorm_after.weight\n",
      "vit.encoder.layer.7.layernorm_after.bias\n",
      "vit.encoder.layer.8.attention.attention.query.weight\n",
      "vit.encoder.layer.8.attention.attention.query.bias\n",
      "vit.encoder.layer.8.attention.attention.key.weight\n",
      "vit.encoder.layer.8.attention.attention.key.bias\n",
      "vit.encoder.layer.8.attention.attention.value.weight\n",
      "vit.encoder.layer.8.attention.attention.value.bias\n",
      "vit.encoder.layer.8.attention.output.dense.weight\n",
      "vit.encoder.layer.8.attention.output.dense.bias\n",
      "vit.encoder.layer.8.intermediate.dense.weight\n",
      "vit.encoder.layer.8.intermediate.dense.bias\n",
      "vit.encoder.layer.8.output.dense.weight\n",
      "vit.encoder.layer.8.output.dense.bias\n",
      "vit.encoder.layer.8.layernorm_before.weight\n",
      "vit.encoder.layer.8.layernorm_before.bias\n",
      "vit.encoder.layer.8.layernorm_after.weight\n",
      "vit.encoder.layer.8.layernorm_after.bias\n",
      "vit.encoder.layer.9.attention.attention.query.weight\n",
      "vit.encoder.layer.9.attention.attention.query.bias\n",
      "vit.encoder.layer.9.attention.attention.key.weight\n",
      "vit.encoder.layer.9.attention.attention.key.bias\n",
      "vit.encoder.layer.9.attention.attention.value.weight\n",
      "vit.encoder.layer.9.attention.attention.value.bias\n",
      "vit.encoder.layer.9.attention.output.dense.weight\n",
      "vit.encoder.layer.9.attention.output.dense.bias\n",
      "vit.encoder.layer.9.intermediate.dense.weight\n",
      "vit.encoder.layer.9.intermediate.dense.bias\n",
      "vit.encoder.layer.9.output.dense.weight\n",
      "vit.encoder.layer.9.output.dense.bias\n",
      "vit.encoder.layer.9.layernorm_before.weight\n",
      "vit.encoder.layer.9.layernorm_before.bias\n",
      "vit.encoder.layer.9.layernorm_after.weight\n",
      "vit.encoder.layer.9.layernorm_after.bias\n",
      "vit.encoder.layer.10.attention.attention.query.weight\n",
      "vit.encoder.layer.10.attention.attention.query.bias\n",
      "vit.encoder.layer.10.attention.attention.key.weight\n",
      "vit.encoder.layer.10.attention.attention.key.bias\n",
      "vit.encoder.layer.10.attention.attention.value.weight\n",
      "vit.encoder.layer.10.attention.attention.value.bias\n",
      "vit.encoder.layer.10.attention.output.dense.weight\n",
      "vit.encoder.layer.10.attention.output.dense.bias\n",
      "vit.encoder.layer.10.intermediate.dense.weight\n",
      "vit.encoder.layer.10.intermediate.dense.bias\n",
      "vit.encoder.layer.10.output.dense.weight\n",
      "vit.encoder.layer.10.output.dense.bias\n",
      "vit.encoder.layer.10.layernorm_before.weight\n",
      "vit.encoder.layer.10.layernorm_before.bias\n",
      "vit.encoder.layer.10.layernorm_after.weight\n",
      "vit.encoder.layer.10.layernorm_after.bias\n",
      "vit.encoder.layer.11.attention.attention.query.weight\n",
      "vit.encoder.layer.11.attention.attention.query.bias\n",
      "vit.encoder.layer.11.attention.attention.key.weight\n",
      "vit.encoder.layer.11.attention.attention.key.bias\n",
      "vit.encoder.layer.11.attention.attention.value.weight\n",
      "vit.encoder.layer.11.attention.attention.value.bias\n",
      "vit.encoder.layer.11.attention.output.dense.weight\n",
      "vit.encoder.layer.11.attention.output.dense.bias\n",
      "vit.encoder.layer.11.intermediate.dense.weight\n",
      "vit.encoder.layer.11.intermediate.dense.bias\n",
      "vit.encoder.layer.11.output.dense.weight\n",
      "vit.encoder.layer.11.output.dense.bias\n",
      "vit.encoder.layer.11.layernorm_before.weight\n",
      "vit.encoder.layer.11.layernorm_before.bias\n",
      "vit.encoder.layer.11.layernorm_after.weight\n",
      "vit.encoder.layer.11.layernorm_after.bias\n",
      "vit.layernorm.weight\n",
      "vit.layernorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for k in model_hf_1.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
